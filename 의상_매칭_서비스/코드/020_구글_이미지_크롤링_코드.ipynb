{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib, urllib.request\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 작은 사이즈로 crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_google_crawling(size, folder, searchItem):\n",
    "    \n",
    "    url = \"https://www.google.com/search\"\n",
    "    webDriver = \"./chromedriver.exe\"\n",
    "    params ={\n",
    "       \"q\":searchItem\n",
    "       ,\"tbm\":\"isch\"\n",
    "       ,\"sa\":\"1\"\n",
    "       ,\"source\":\"lnms&tbm=isch\"\n",
    "    }\n",
    "    \n",
    "    url = url+\"?\"+urllib.parse.urlencode(params)\n",
    "    browser = webdriver.Chrome(webDriver)\n",
    "    time.sleep(0.5)\n",
    "    browser.get(url)\n",
    "    html = browser.page_source\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    soup_temp = BeautifulSoup(html,'html.parser')\n",
    "    img4page = len(soup_temp.findAll(\"img\"))\n",
    "\n",
    "    elem = browser.find_element_by_tag_name(\"body\")\n",
    "    imgCnt =0\n",
    "    while imgCnt < size*10:\n",
    "        elem.send_keys(Keys.PAGE_DOWN)\n",
    "        rnd = random.random()\n",
    "        #print(imgCnt)\n",
    "        time.sleep(rnd)\n",
    "        imgCnt+=img4page\n",
    "        \n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    img = soup.findAll(\"img\")\n",
    "\n",
    "    browser.find_elements_by_tag_name('img')\n",
    "\n",
    "    fileNum=0\n",
    "    srcURL=[]\n",
    "\n",
    "    for line in img:\n",
    "       if str(line).find('data-src') != -1 and str(line).find('http')<100:  \n",
    "          #print(fileNum, \" : \", line['data-src'])  \n",
    "          srcURL.append(line['data-src'])\n",
    "          fileNum+=1\n",
    "            \n",
    "\n",
    "    saveDir = folder+searchItem\n",
    "\n",
    "    try:\n",
    "        if not(os.path.isdir(saveDir)):\n",
    "            os.makedirs(os.path.join(saveDir))\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            print(\"Failed to create directory!!!!!\")\n",
    "            raise\n",
    "\n",
    "    for i,src in zip(range(fileNum),srcURL):\n",
    "        urllib.request.urlretrieve(src, saveDir+\"/\"+str(i)+\".jpg\")\n",
    "        if i % 50 == 0:\n",
    "            print(i,\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인\n",
    "\n",
    "folder = \"C:/python_DA/img/\"   # crawling 되는 이미지 폴더 만들 path\n",
    "searchItem = \"공항패션\"        # keyword\n",
    "size = 300                     # 갯수\n",
    "\n",
    "small_google_crawling(size, folder, searchItem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 큰 사이즈로 crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_google_crawling(size, folder_dir, searchItem):\n",
    "    url = 'https://www.google.com/search?q=' + searchItem + '&tbm=isch'\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    elem = driver.find_element_by_tag_name(\"body\")\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(folder_dir):\n",
    "            os.makedirs(folder_dir)\n",
    "    except OSError:\n",
    "        print('Error Creating directory'+folder_dir)\n",
    "    \n",
    "\n",
    "    for i in range(1,size):\n",
    "        ### 이미지 클릭\n",
    "        try:\n",
    "            click_section = '//*[@id=\"islrg\"]/div[1]/div[' + str(i) + ']/a[1]'  \n",
    "            #div[이 안의 숫자]가 바뀌면은 클릭할 사진이 바뀌는 거임\n",
    "            driver.find_element_by_xpath(click_section).click()\n",
    "            time.sleep(2)\n",
    "\n",
    "        ### 오른쪽에 뜬 이미지 url 가져오기\n",
    "            img_section = '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/div/div[1]/div[1]/div/div[2]/a/img'\n",
    "            img = driver.find_element_by_xpath(img_section).get_attribute('src')\n",
    "\n",
    "        ### 이미지 저장\n",
    "            img_name = folder_dir + '/' + searchItem + '원본s' + str(i) + '.jpg'\n",
    "            urllib.request.urlretrieve(img,img_name)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if size % 100 == 0:\n",
    "        print(size)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 300\n",
    "folder_dir = 'C:/python_DA/img/데일리룩'\n",
    "searchItem = '데일리룩'\n",
    "big_google_crawling(size, folder_dir, searchItem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
